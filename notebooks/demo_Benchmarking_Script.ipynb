{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>How to run this notebook?</b><br />\n",
    "<ol>\n",
    "    <li>Install the DockStream environment: conda env create -f environment.yml in the DockStream directory</li>\n",
    "    <li>Activate the environment: conda activate DockStreamCommunity</li>\n",
    "    <li>Execute jupyter: jupyter notebook</li>\n",
    "    <li> Copy the link to a browser</li>\n",
    "    <li> Update variables <b>dockstream_path</b> and <b>dockstream_env</b> (the path to the environment DockStream) in the \n",
    "        first code block below</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n",
    "# Benchmarking Script Demo\n",
    "\n",
    "The purpose of the `benchmarking script` is to enable automated batch execution of `DockStream` runs. This will allow users to run multiple backends + ligand embedders (e.g. `Glide with LigPrep` and `Hybrid with Corina`) in a more streamlined manner to determine the best docking configuration for their specific application. A subsequent `analysis script` quantifies the differences between different docking configurations by automating calculations of relevant enrichment metrics and generating plots for visualization of `DockStream` run results. A demo for the `analysis script` can be found in the `demo_Analysis_Script` Jupyter notebook. This notebook focuses strictly on the `benchmarking script` and demonstrates the necessary preparatory steps for batch execution of `DockStream` runs. \n",
    "\n",
    "\n",
    "**Benchmarking Script Steps:**\n",
    "  1. Prepare the ligands file\n",
    "  2. Prepare the receptors/grids\n",
    "  3. Prepare `DockStream` configuration files (`JSON` format)\n",
    "  4. Execute the script and parse the results\n",
    "\n",
    "\n",
    "__Note:__ By default, this notebook will deposit all files created into `~/Desktop/Benchmarking_demo`.\n",
    "\n",
    "The following imports / loadings are only necessary when executing this notebook. If you want to use `benchmarking_v2.py` directly from the command-line, it is enough to execute the following with the appropriate input path (path to a folder containing `DockStream` configuration `JSONs`):\n",
    "\n",
    "```\n",
    "conda activate DockStream (or DockStreamFull for GOLD docking)\n",
    "python /path/to/DockStream/benchmarking.py -input_path <path to input JSONs folder>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "# update these paths to reflect your system's configuration\n",
    "dockstream_path = os.path.expanduser(\"~/Desktop/ProjectData/DockStream\")\n",
    "# note: DockStreamFull (as opposed to DockStream) is required to run GOLD\n",
    "dockstream_env = os.path.expanduser(\"~/miniconda3/envs/DockStream\")\n",
    "\n",
    "# no changes are necessary beyond this point\n",
    "# ---------\n",
    "# get the notebook's root path\n",
    "try: ipynb_path\n",
    "except NameError: ipynb_path = os.getcwd()\n",
    "\n",
    "# generate the path to the benchmarking script entry point\n",
    "benchmarking_script = os.path.join(dockstream_path, \"benchmarking.py\")\n",
    "\n",
    "# generate a folder to store the results\n",
    "output_dir = os.path.expanduser(\"~/Desktop/Benchmarking_demo\")\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the Ligands File\n",
    "\n",
    "A file containing all the ligands to be docked must be generated in either `SDF` or `SMI` or `CSV` format (see docking_input_types notebook for more details). Typically, `SMI` format is used for readability especially when there is a substantial number of ligands to be docked. In the `DockStream` codebase, there is a script called `sdf2smiles.py` which converts a ligands database in `SDF` format to `SMI` format. \n",
    "\n",
    "For the purpose of this notebook, a ligand `SMI` file is provided and shipped in the `DockStream` codebase. Let's generate the path to this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the paths to the ligands smi file shipped with this implementation\n",
    "ligands_path = ipynb_path + \"/../data/Benchmarking_Script/ligands_smiles.smi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Receptors/Grids\n",
    "\n",
    "As with any `DockStream` run, a receptor/grid must be prepared before the docking run. This process will be dependent on the backend(s) used. For example, receptor grids for `Glide` can be generated using Schrodinger's GUI, `Maestro` (see demo_Glide for more details). On the other hand, receptor grids for `Hybrid` can be generated using `target_preparator.py` which is a script shipped with the `DockStream` codebase (see demo_Hybrid for more details).\n",
    "\n",
    "For the purpose of this notebook, Glide and Hybrid receptor grids are provided and shipped with the `DockStream` codebase. Note that the benchmarking script is compatible with any backend and any ligand embedder. `Glide` and `Hybrid` were chosen in this notebook arbitrarily. Let's generate the paths to the relevant files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the paths to the receptor grids shipped with this implementation\n",
    "glide_grid_path = os.path.join(ipynb_path, \"../data/Benchmarking_Script/1UYD_grid.zip\")\n",
    "hybrid_grid_path = os.path.join(ipynb_path, \"../data/Benchmarking_Script/1UYD_grid.oeb\")\n",
    "smiles_path = os.path.join(ipynb_path, \"../data/Benchmarking_Script/ligands_smiles.smi\")\n",
    "\n",
    "# generate output paths for the docked ligands and the scores\n",
    "glide_docked_poses_path = os.path.join(output_dir, \"glide_docked_poses.sdf\")\n",
    "glide_docked_scores_path = os.path.join(output_dir, \"glide_docked_scores.csv\")\n",
    "hybrid_docked_poses_path = os.path.join(output_dir, \"hybrid_docked_poses.sdf\")\n",
    "hybrid_docked_scores_path = os.path.join(output_dir, \"hybrid_docked_scores.csv\")                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the above code block the `benchmarking_run_jsons` path is a path to a folder rather than a single `DockStream` `JSON` configuration file. This is because the `benchmarking script` takes as input a folder containing 1 or more `DockStream` `JSON` configuration files and runs them all successively (single runs are supported too, in which case, the path to the single configuration `JSON` should be passed). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare the DockStream Configuration Files\n",
    "\n",
    "Next, we need to generate the `DockStream` configuration files. This step is covered in the backend specific demos (e.g. `demo_Glide`) but will be briefly described again here as it is especially relevant in highlighting the utility of the benchmarking script. Let's first create a new subfolder to hold the `DockStream` run JSONs that will be generated later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output paths for the DockStream configuration files \n",
    "benchmarking_conf_jsons = os.path.join(output_dir, \"benchmarking_conf_jsons\")\n",
    "# create the benchmaking run jsons subfolder\n",
    "try:\n",
    "    os.mkdir(benchmarking_conf_jsons)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `DockStream` run will require its own configuration `JSON` (they need not be unique but that would simply run `DockStream` with the exact same configuration which is probably not desirable unless you are interested in observing the stochastic nature of some docking algorithms in select backends such as `GOLD`). An example `Glide with LigPrep` configuration `JSON` is shown in the below code block (for more details on `Glide`, see demo_Glide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the embedding and docking JSON file as a dictionary and write it out\n",
    "glide_ligprep_conf_json = {\n",
    "  \"docking\": {\n",
    "    \"header\": {                                   # general settings\n",
    "      \"environment\": {\n",
    "      }\n",
    "    },\n",
    "    \"ligand_preparation\": {                       # the ligand preparation part, defines how to build the pool\n",
    "      \"embedding_pools\": [\n",
    "        {\n",
    "          \"pool_id\": \"Ligprep\",\n",
    "          \"type\": \"Ligprep\",\n",
    "          \"parameters\": {\n",
    "            \"prefix_execution\": \"module load schrodinger/2019-4\",\n",
    "            \"use_epik\": {\n",
    "                \"target_pH\": 7.4,                 # LigPrep embeds ligands at a specified pH which is particularly\n",
    "                \"pH_tolerance\": 0.2               # relevant to ionization states --> this parameter can be tweaked\n",
    "          },\n",
    "            \"force_field\": \"OPLS3e\"\n",
    "          },\n",
    "          \"input\": {\n",
    "            \"standardize_smiles\": False,\n",
    "            \"input_path\": smiles_path,\n",
    "            \"type\": \"smi\"                                   \n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"docking_runs\": [\n",
    "        {\n",
    "          \"backend\": \"Glide\",\n",
    "          \"run_id\": \"Glide\",\n",
    "        \"input_pools\": [\"Ligprep\"],\n",
    "        \"parameters\": {\n",
    "          \"prefix_execution\": \"module load schrodinger/2019-4\", # will be executed before a program call\n",
    "          \"parallelization\": {                                  \n",
    "            \"number_cores\": 2\n",
    "          },\n",
    "          \"glide_flags\": {                                  # all all command-line flags for Glide here \n",
    "            \"-HOST\": \"localhost\"\n",
    "          },\n",
    "          \"glide_keywords\": {                               # add all keywords for the \"input.in\" file here\n",
    "            \"EXPANDED_SAMPLING\": \"True\",                    # all these parameteres can be tweaked and/or\n",
    "            \"GRIDFILE\": [glide_grid_path],                  # included/omitted\n",
    "            \"NENHANCED_SAMPLING\": \"2\",\n",
    "            \"POSE_OUTTYPE\": \"ligandlib_sd\",\n",
    "            \"POSES_PER_LIG\": \"3\",\n",
    "            \"POSTDOCK_NPOSE\": \"15\",\n",
    "            \"POSTDOCKSTRAIN\": \"True\",\n",
    "            \"PRECISION\": \"HTVS\"\n",
    "          }\n",
    "        },\n",
    "        \"output\": {\n",
    "          \"poses\": { \"poses_path\": glide_docked_poses_path },            # output path to save docked poses\n",
    "          \"scores\": { \"scores_path\": glide_docked_scores_path }          # output path to save docked scores   \n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(os.path.join(benchmarking_conf_jsons, \"Glide_LigPrep.json\"), \"w+\") as f:\n",
    "    json.dump(glide_ligprep_conf_json, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell block saves the `DockStream` `Glide` configuration `JSON` in the benchmarking `JSONs` folder. Notice the comments that highlight parameters that can be tweaked. For instance, `\"target_pH\"` can be tweaked if the user is interested in docking a set of ligands at different pH. This can have a significant impact on ligand activity as the ionization states will change. Moreover, one could envision changing parameters located under `\"glide_keywords\"`. For instance, the `\"PRECISION\"` can be changed to `\"SP\" (\"Standard Precision\")` which is generally more accurate than `\"HTVS\" (\"High Throughput Virtual Screening\")` which is only used in this notebook simply because it is much faster. One can change as many or as few parameters as they would like. It is evident that the combinations of parameters leads to a combinatorial explosion of docking configurations. In the event that the user wants to run many `DockStream` runs, it would be cumbersome to keep executing `docker.py`. The utility of the `benchmarking script` is to automate running all `DockStream` jobs so long as the configuration `JSON` is provided. Internally, the script calls `docker.py` and therefore no functionalities are lost in using the benchmarking script.\n",
    "\n",
    "As the purpose of this notebook is to demonstrate batch execution of `DockStream` runs, the below code block will generate an example `Hybrid with Corina` configuration `JSON` (for more details on `Hybrid`, see demo_Hybrid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_corina_conf_json = {\n",
    "  \"docking\": {\n",
    "    \"header\": {\n",
    "       \"environment\":{\n",
    "        }\n",
    "      },\n",
    "    \"ligand_preparation\": {\n",
    "      \"embedding_pools\": [\n",
    "        {\n",
    "          \"pool_id\": \"Corina\",                                      # Corina is used here as the ligand embedder\n",
    "          \"type\": \"Corina\",                                         # but this can be changed to LigPrep or RDKit\n",
    "          \"parameters\": {\n",
    "              \"prefix_execution\": \"module load corina\"\n",
    "          },\n",
    "          \"input\": {\n",
    "            \"standardize_smiles\": False,\n",
    "            \"input_path\": ligands_path,\n",
    "            \"type\": \"smi\"\n",
    "           }\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"docking_runs\": [\n",
    "      {\n",
    "        \"backend\": \"Hybrid\",\n",
    "        \"run_id\": \"Hybrid\",\n",
    "        \"input_pools\": [\"Corina\"],\n",
    "        \"parameters\": {\n",
    "          \"prefix_execution\": \"module load oedocking\",\n",
    "          \"parallelization\": {\n",
    "            \"number_cores\": 2\n",
    "          },\n",
    "          \"receptor_paths\": [hybrid_grid_path]\n",
    "        },\n",
    "        \"output\": {\n",
    "          \"poses\": { \"poses_path\": hybrid_docked_poses_path },            # output path to save docked poses\n",
    "          \"scores\": { \"scores_path\": hybrid_docked_scores_path }          # output path to save docked scores   \n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(os.path.join(benchmarking_conf_jsons, \"Hybrid_Corina.json\"), \"w+\") as f:\n",
    "    json.dump(hybrid_corina_conf_json, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell block saves the `DockStream` `Hybrid` configuration `JSON` in the benchmarking `JSON` folder. Note that `Hybrid` has much fewer parameters that can be tweaked compared to `Glide`. \n",
    "\n",
    "We are now finished generating the `DockStream` configuration `JSONs`. There is no limit to how many configuration `JSONs` are provided; the `benchmarking script` will continue running `DockStream` until all configuration JSONs are executed. For the purpose of this notebook, only the 2 runs specified above (`Glide with LigPrep` and `Hybrid with Corina`) will be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Execute the Benchmarking Script\n",
    "\n",
    "We are now ready to execute batch `DockStream` runs. Call the `benchmarking script` via command-line and provide the `-input path` argument which is the path to the folder containing all the `DockStream` configuration `JSONs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this in a command-line environment after replacing the parameters\n",
    "!{dockstream_env}/bin/python {benchmarking_script} -input_path {benchmarking_conf_jsons}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any `DockStream` run, the docked poses and scores are outputted to `SDF` and `CSV` files as specified in the configuration `JSONs`. As a final note, the `benchmarking script` will output an error message if an invalid path is provided and will also notify the user which `DockStream` run failed with the associated error trace back displayed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
